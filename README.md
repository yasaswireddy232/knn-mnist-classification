# KNN Classifier & MNIST Digit Classification

This project implements a **Kâ€‘Nearest Neighbors (KNN) classifier from scratch** in Python to:
- Explore effects of different values of *k* and distance metrics (L1 / L2)
- Visualize decision boundaries and performance on synthetic datasets (Gaussian & Laplacian)
- Apply the classifier to the MNIST handwritten digit dataset (~70,000 images)



---

##  **Project Highlights**
- Built custom `KNeighborsClassifier` without using scikit-learn's built-in KNN.
- Performed hyperparameter tuning via cross-validation to select the best *k*.
- Applied to MNIST data, achieving ~XX% test accuracy.
- Created visualizations: decision regions, confusion matrix, and error plots.
- Investigated how data distribution and distance metrics affect model performance.

---





- Developed a custom KNN classifier and evaluated it on real-world data (MNIST) achieving ~91% test accuracy.
- Visualized decision boundaries and analyzed confusion matrix to identify commonly misclassified digits.

---

## ðŸš€ **Possible improvements**
- Dimensionality reduction with PCA
- Weighted KNN (give closer neighbors higher influence)
- Feature engineering (e.g., edge detection, gradients)
- Additional distance metrics (cosine, Mahalanobis)

---

